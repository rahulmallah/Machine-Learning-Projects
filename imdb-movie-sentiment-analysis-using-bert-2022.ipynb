{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install ktrain","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:18:13.753648Z","iopub.execute_input":"2022-07-10T10:18:13.754594Z","iopub.status.idle":"2022-07-10T10:18:52.218340Z","shell.execute_reply.started":"2022-07-10T10:18:13.754519Z","shell.execute_reply":"2022-07-10T10:18:52.217293Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting ktrain\n  Downloading ktrain-0.31.2-py3-none-any.whl (25.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting whoosh\n  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from ktrain) (21.3)\nCollecting scikit-learn==0.24.2\n  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from ktrain) (1.3.5)\nCollecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: fastprogress>=0.1.21 in /opt/conda/lib/python3.7/site-packages (from ktrain) (1.0.2)\nCollecting cchardet\n  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.7/263.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from ktrain) (1.1.0)\nRequirement already satisfied: chardet in /opt/conda/lib/python3.7/site-packages (from ktrain) (4.0.0)\nRequirement already satisfied: matplotlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from ktrain) (3.5.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from ktrain) (2.27.1)\nCollecting keras-bert>=0.86.0\n  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: jieba in /opt/conda/lib/python3.7/site-packages (from ktrain) (0.42.1)\nCollecting syntok==1.3.3\n  Downloading syntok-1.3.3-py3-none-any.whl (22 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from ktrain) (0.1.96)\nCollecting transformers==4.10.3\n  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.2->ktrain) (1.7.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.2->ktrain) (3.1.0)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.2->ktrain) (1.21.6)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from syntok==1.3.3->ktrain) (2021.11.10)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (0.0.53)\nRequirement already satisfied: huggingface-hub>=0.0.12 in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (0.7.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (4.64.0)\nCollecting tokenizers<0.11,>=0.10.1\n  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (6.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (4.11.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.10.3->ktrain) (3.6.0)\nCollecting keras-transformer==0.40.0\n  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-pos-embd==0.13.0\n  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-multi-head==0.29.0\n  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-layer-normalization==0.16.0\n  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-position-wise-feed-forward==0.8.0\n  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-embed-sim==0.10.0\n  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-self-attention==0.51.0\n  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (4.33.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (9.1.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (3.0.9)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.0->ktrain) (1.4.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.1->ktrain) (2022.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from langdetect->ktrain) (1.16.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (1.26.9)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (2.0.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (2022.6.15)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->ktrain) (3.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.0.12->transformers==4.10.3->ktrain) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.10.3->ktrain) (3.8.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.10.3->ktrain) (8.0.4)\nBuilding wheels for collected packages: keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect\n  Building wheel for keras-bert (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=6ae417c9c854423bfbfc7026e914263fa5fe6a390aefd38db3310089fe96734c\n  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n  Building wheel for keras-transformer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=733965766d98e958d296d6a3f3640c321ac706206679a4944f1d8a29bfd1157f\n  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=c175b139c72bcfe58b4a1a545f31ad909ae1bf6ddff6f3f3207c6694bdd1981b\n  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=a48aeb5b534d39c615710a22e5e1b627be775ceec47bd150e70a11ada6ef50ff\n  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n  Building wheel for keras-multi-head (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=f898e3b68fb418b17c2ebcd186643568c25de0eacbf8a5386bb56877fbba0b4a\n  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=d1eadf6bc64a2bcd2e57e9408a3ead5aa5761a44c19e887243eb05d250290a48\n  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=ab944ffa18348c038ec7fd343cdcfd2da57a65944a4777cdd2c7214be0ab154c\n  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=dcaa63d598d87c37069a502937d20be01d313163a6f1d2f1d54f958c94710d0e\n  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=d3b534bf20de92c43b6704ce39da8090b62caf7ca202bf56b543804a400a29f6\n  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\nSuccessfully built keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect\nInstalling collected packages: whoosh, tokenizers, cchardet, syntok, langdetect, keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, scikit-learn, keras-multi-head, keras-transformer, transformers, keras-bert, ktrain\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.12.1\n    Uninstalling tokenizers-0.12.1:\n      Successfully uninstalled tokenizers-0.12.1\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.0.2\n    Uninstalling scikit-learn-1.0.2:\n      Successfully uninstalled scikit-learn-1.0.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.18.0\n    Uninstalling transformers-4.18.0:\n      Successfully uninstalled transformers-4.18.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nyellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.2 which is incompatible.\nmlxtend 0.20.0 requires scikit-learn>=1.0.2, but you have scikit-learn 0.24.2 which is incompatible.\nimbalanced-learn 0.9.0 requires scikit-learn>=1.0.1, but you have scikit-learn 0.24.2 which is incompatible.\ngplearn 0.4.2 requires scikit-learn>=1.0.2, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cchardet-2.1.7 keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 ktrain-0.31.2 langdetect-1.0.9 scikit-learn-0.24.2 syntok-1.3.3 tokenizers-0.10.3 transformers-4.10.3 whoosh-2.7.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport ktrain\nfrom ktrain import text","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:18:52.220591Z","iopub.execute_input":"2022-07-10T10:18:52.221197Z","iopub.status.idle":"2022-07-10T10:18:59.416687Z","shell.execute_reply.started":"2022-07-10T10:18:52.221156Z","shell.execute_reply":"2022-07-10T10:18:59.415412Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/imdb-movie-ratings-sentiment-analysis/movie.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:18:59.418293Z","iopub.execute_input":"2022-07-10T10:18:59.419187Z","iopub.status.idle":"2022-07-10T10:19:00.404373Z","shell.execute_reply.started":"2022-07-10T10:18:59.419157Z","shell.execute_reply":"2022-07-10T10:19:00.403444Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0  I grew up (b. 1965) watching and loving the Th...      0\n1  When I put this movie in my DVD player, and sa...      0\n2  Why do people who do not know what a particula...      0\n3  Even though I have great interest in Biblical ...      0\n4  Im a die hard Dads Army fan and nothing will e...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I grew up (b. 1965) watching and loving the Th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>When I put this movie in my DVD player, and sa...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Why do people who do not know what a particula...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Even though I have great interest in Biblical ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Im a die hard Dads Army fan and nothing will e...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:19:00.407098Z","iopub.execute_input":"2022-07-10T10:19:00.407715Z","iopub.status.idle":"2022-07-10T10:19:00.414668Z","shell.execute_reply.started":"2022-07-10T10:19:00.407677Z","shell.execute_reply":"2022-07-10T10:19:00.413601Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(40000, 2)"},"metadata":{}}]},{"cell_type":"code","source":"data_train=data.head(30000)\ndata_train","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:19:00.416314Z","iopub.execute_input":"2022-07-10T10:19:00.416708Z","iopub.status.idle":"2022-07-10T10:19:00.430884Z","shell.execute_reply.started":"2022-07-10T10:19:00.416651Z","shell.execute_reply":"2022-07-10T10:19:00.429833Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                    text  label\n0      I grew up (b. 1965) watching and loving the Th...      0\n1      When I put this movie in my DVD player, and sa...      0\n2      Why do people who do not know what a particula...      0\n3      Even though I have great interest in Biblical ...      0\n4      Im a die hard Dads Army fan and nothing will e...      1\n...                                                  ...    ...\n29995  Monstervision was a show I grew up with. From ...      1\n29996  SPOILERS<br /><br />A buddy of mine said NEXT ...      1\n29997  This was a great romantic comedy! Historically...      1\n29998  You've gotta hand it to Steven Seagal: whateve...      0\n29999  In the old commercial for blank audio cassette...      1\n\n[30000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I grew up (b. 1965) watching and loving the Th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>When I put this movie in my DVD player, and sa...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Why do people who do not know what a particula...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Even though I have great interest in Biblical ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Im a die hard Dads Army fan and nothing will e...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>29995</th>\n      <td>Monstervision was a show I grew up with. From ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29996</th>\n      <td>SPOILERS&lt;br /&gt;&lt;br /&gt;A buddy of mine said NEXT ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29997</th>\n      <td>This was a great romantic comedy! Historically...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29998</th>\n      <td>You've gotta hand it to Steven Seagal: whateve...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29999</th>\n      <td>In the old commercial for blank audio cassette...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>30000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_test=data.tail(10000)\ndata_test","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:19:00.432348Z","iopub.execute_input":"2022-07-10T10:19:00.432685Z","iopub.status.idle":"2022-07-10T10:19:00.448566Z","shell.execute_reply.started":"2022-07-10T10:19:00.432652Z","shell.execute_reply":"2022-07-10T10:19:00.446879Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                    text  label\n30000  Supposedly, a movie about a magazine sending j...      0\n30001  I really enjoyed this movie... In My DVD colle...      1\n30002  It is sad that Schwarzenegger was the best thi...      0\n30003  Terry West had a good idea w\\ this movie. He j...      0\n30004  An accurate review of nuremburg must consider ...      0\n...                                                  ...    ...\n39995  \"Western Union\" is something of a forgotten cl...      1\n39996  This movie is an incredible piece of work. It ...      1\n39997  My wife and I watched this movie because we pl...      0\n39998  When I first watched Flatliners, I was amazed....      1\n39999  Why would this film be so good, but only gross...      1\n\n[10000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>30000</th>\n      <td>Supposedly, a movie about a magazine sending j...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30001</th>\n      <td>I really enjoyed this movie... In My DVD colle...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30002</th>\n      <td>It is sad that Schwarzenegger was the best thi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30003</th>\n      <td>Terry West had a good idea w\\ this movie. He j...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30004</th>\n      <td>An accurate review of nuremburg must consider ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39995</th>\n      <td>\"Western Union\" is something of a forgotten cl...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39996</th>\n      <td>This movie is an incredible piece of work. It ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39997</th>\n      <td>My wife and I watched this movie because we pl...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39998</th>\n      <td>When I first watched Flatliners, I was amazed....</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39999</th>\n      <td>Why would this film be so good, but only gross...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"(x_train,y_train),(x_test,y_test),preprocess = text.texts_from_df(train_df=data_train,text_column='text',label_columns='label',\n                                                                  val_df=data_test,maxlen=399,preprocess_mode='bert')","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:19:00.450510Z","iopub.execute_input":"2022-07-10T10:19:00.451251Z","iopub.status.idle":"2022-07-10T10:21:36.567868Z","shell.execute_reply.started":"2022-07-10T10:19:00.451215Z","shell.execute_reply":"2022-07-10T10:21:36.566836Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['not_label', 'label']\n   not_label  label\n0        1.0    0.0\n1        1.0    0.0\n2        1.0    0.0\n3        1.0    0.0\n4        0.0    1.0\n['not_label', 'label']\n       not_label  label\n30000        1.0    0.0\n30001        0.0    1.0\n30002        1.0    0.0\n30003        1.0    0.0\n30004        1.0    0.0\ndownloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n[██████████████████████████████████████████████████]\nextracting pretrained BERT model...\ndone.\n\ncleanup downloaded zip...\ndone.\n\npreprocessing train...\nlanguage: en\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"done."},"metadata":{}},{"name":"stdout","text":"Is Multi-Label? False\npreprocessing test...\nlanguage: en\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"done."},"metadata":{}}]},{"cell_type":"code","source":"x_train","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:21:36.569531Z","iopub.execute_input":"2022-07-10T10:21:36.569900Z","iopub.status.idle":"2022-07-10T10:21:36.578723Z","shell.execute_reply.started":"2022-07-10T10:21:36.569865Z","shell.execute_reply":"2022-07-10T10:21:36.577434Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[array([[ 101, 1045, 3473, ...,    0,    0,    0],\n        [ 101, 2043, 1045, ..., 8343, 2009,  102],\n        [ 101, 2339, 2079, ...,    0,    0,    0],\n        ...,\n        [ 101, 2023, 2001, ...,    0,    0,    0],\n        [ 101, 2017, 1005, ...,    0,    0,    0],\n        [ 101, 1999, 1996, ..., 2041, 2045,  102]]),\n array([[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]])]"},"metadata":{}}]},{"cell_type":"code","source":"x_test[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:21:36.579985Z","iopub.execute_input":"2022-07-10T10:21:36.580939Z","iopub.status.idle":"2022-07-10T10:21:36.589563Z","shell.execute_reply.started":"2022-07-10T10:21:36.580895Z","shell.execute_reply":"2022-07-10T10:21:36.588574Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(10000, 399)"},"metadata":{}}]},{"cell_type":"code","source":"x_train[1].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:21:36.594580Z","iopub.execute_input":"2022-07-10T10:21:36.595130Z","iopub.status.idle":"2022-07-10T10:21:36.601741Z","shell.execute_reply.started":"2022-07-10T10:21:36.595094Z","shell.execute_reply":"2022-07-10T10:21:36.600718Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(30000, 399)"},"metadata":{}}]},{"cell_type":"code","source":"y_train[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:21:36.603141Z","iopub.execute_input":"2022-07-10T10:21:36.603919Z","iopub.status.idle":"2022-07-10T10:21:36.614444Z","shell.execute_reply.started":"2022-07-10T10:21:36.603884Z","shell.execute_reply":"2022-07-10T10:21:36.613595Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(2,)"},"metadata":{}}]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:21:36.616108Z","iopub.execute_input":"2022-07-10T10:21:36.616872Z","iopub.status.idle":"2022-07-10T10:21:36.625559Z","shell.execute_reply.started":"2022-07-10T10:21:36.616836Z","shell.execute_reply":"2022-07-10T10:21:36.624712Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([[1., 0.],\n       [0., 1.],\n       [1., 0.],\n       ...,\n       [1., 0.],\n       [0., 1.],\n       [0., 1.]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"model=text.text_classifier(name='bert',train_data=(x_train,y_train),preproc=preprocess)","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:21:36.628107Z","iopub.execute_input":"2022-07-10T10:21:36.629055Z","iopub.status.idle":"2022-07-10T10:21:46.357737Z","shell.execute_reply.started":"2022-07-10T10:21:36.629019Z","shell.execute_reply":"2022-07-10T10:21:46.356737Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Is Multi-Label? False\nmaxlen is 399\ndone.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:21:46.358989Z","iopub.execute_input":"2022-07-10T10:21:46.359665Z","iopub.status.idle":"2022-07-10T10:21:46.378132Z","shell.execute_reply.started":"2022-07-10T10:21:46.359625Z","shell.execute_reply":"2022-07-10T10:21:46.377238Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nInput-Token (InputLayer)        [(None, 399)]        0                                            \n__________________________________________________________________________________________________\nInput-Segment (InputLayer)      [(None, 399)]        0                                            \n__________________________________________________________________________________________________\nEmbedding-Token (TokenEmbedding [(None, 399, 768), ( 23440896    Input-Token[0][0]                \n__________________________________________________________________________________________________\nEmbedding-Segment (Embedding)   (None, 399, 768)     1536        Input-Segment[0][0]              \n__________________________________________________________________________________________________\nEmbedding-Token-Segment (Add)   (None, 399, 768)     0           Embedding-Token[0][0]            \n                                                                 Embedding-Segment[0][0]          \n__________________________________________________________________________________________________\nEmbedding-Position (PositionEmb (None, 399, 768)     306432      Embedding-Token-Segment[0][0]    \n__________________________________________________________________________________________________\nEmbedding-Dropout (Dropout)     (None, 399, 768)     0           Embedding-Position[0][0]         \n__________________________________________________________________________________________________\nEmbedding-Norm (LayerNormalizat (None, 399, 768)     1536        Embedding-Dropout[0][0]          \n__________________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAttentio (None, 399, 768)     2362368     Embedding-Norm[0][0]             \n__________________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-1-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAttentio (None, 399, 768)     0           Embedding-Norm[0][0]             \n                                                                 Encoder-1-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAttentio (None, 399, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-1-FeedForward (FeedForw (None, 399, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-1-FeedForward-Dropout ( (None, 399, 768)     0           Encoder-1-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-1-FeedForward-Add (Add) (None, 399, 768)     0           Encoder-1-MultiHeadSelfAttention-\n                                                                 Encoder-1-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-1-FeedForward-Norm (Lay (None, 399, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAttentio (None, 399, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-2-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n                                                                 Encoder-2-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAttentio (None, 399, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-2-FeedForward (FeedForw (None, 399, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-2-FeedForward-Dropout ( (None, 399, 768)     0           Encoder-2-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-2-FeedForward-Add (Add) (None, 399, 768)     0           Encoder-2-MultiHeadSelfAttention-\n                                                                 Encoder-2-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-2-FeedForward-Norm (Lay (None, 399, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAttentio (None, 399, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-3-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n                                                                 Encoder-3-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAttentio (None, 399, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-3-FeedForward (FeedForw (None, 399, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-3-FeedForward-Dropout ( (None, 399, 768)     0           Encoder-3-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-3-FeedForward-Add (Add) (None, 399, 768)     0           Encoder-3-MultiHeadSelfAttention-\n                                                                 Encoder-3-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-3-FeedForward-Norm (Lay (None, 399, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAttentio (None, 399, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-4-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n                                                                 Encoder-4-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAttentio (None, 399, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-4-FeedForward (FeedForw (None, 399, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-4-FeedForward-Dropout ( (None, 399, 768)     0           Encoder-4-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-4-FeedForward-Add (Add) (None, 399, 768)     0           Encoder-4-MultiHeadSelfAttention-\n                                                                 Encoder-4-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-4-FeedForward-Norm (Lay (None, 399, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAttentio (None, 399, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-5-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n                                                                 Encoder-5-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAttentio (None, 399, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-5-FeedForward (FeedForw (None, 399, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-5-FeedForward-Dropout ( (None, 399, 768)     0           Encoder-5-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-5-FeedForward-Add (Add) (None, 399, 768)     0           Encoder-5-MultiHeadSelfAttention-\n                                                                 Encoder-5-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-5-FeedForward-Norm (Lay (None, 399, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAttentio (None, 399, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-6-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n                                                                 Encoder-6-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAttentio (None, 399, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-6-FeedForward (FeedForw (None, 399, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-6-FeedForward-Dropout ( (None, 399, 768)     0           Encoder-6-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-6-FeedForward-Add (Add) (None, 399, 768)     0           Encoder-6-MultiHeadSelfAttention-\n                                                                 Encoder-6-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-6-FeedForward-Norm (Lay (None, 399, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAttentio (None, 399, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-7-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n                                                                 Encoder-7-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAttentio (None, 399, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-7-FeedForward (FeedForw (None, 399, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-7-FeedForward-Dropout ( (None, 399, 768)     0           Encoder-7-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-7-FeedForward-Add (Add) (None, 399, 768)     0           Encoder-7-MultiHeadSelfAttention-\n                                                                 Encoder-7-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-7-FeedForward-Norm (Lay (None, 399, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAttentio (None, 399, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-8-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n                                                                 Encoder-8-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAttentio (None, 399, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-8-FeedForward (FeedForw (None, 399, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-8-FeedForward-Dropout ( (None, 399, 768)     0           Encoder-8-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-8-FeedForward-Add (Add) (None, 399, 768)     0           Encoder-8-MultiHeadSelfAttention-\n                                                                 Encoder-8-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-8-FeedForward-Norm (Lay (None, 399, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAttentio (None, 399, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-9-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAttentio (None, 399, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n                                                                 Encoder-9-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAttentio (None, 399, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-9-FeedForward (FeedForw (None, 399, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-9-FeedForward-Dropout ( (None, 399, 768)     0           Encoder-9-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-9-FeedForward-Add (Add) (None, 399, 768)     0           Encoder-9-MultiHeadSelfAttention-\n                                                                 Encoder-9-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-9-FeedForward-Norm (Lay (None, 399, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAttenti (None, 399, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAttenti (None, 399, 768)     0           Encoder-10-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAttenti (None, 399, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n                                                                 Encoder-10-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAttenti (None, 399, 768)     1536        Encoder-10-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-10-FeedForward (FeedFor (None, 399, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-10-FeedForward-Dropout  (None, 399, 768)     0           Encoder-10-FeedForward[0][0]     \n__________________________________________________________________________________________________\nEncoder-10-FeedForward-Add (Add (None, 399, 768)     0           Encoder-10-MultiHeadSelfAttention\n                                                                 Encoder-10-FeedForward-Dropout[0]\n__________________________________________________________________________________________________\nEncoder-10-FeedForward-Norm (La (None, 399, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n__________________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAttenti (None, 399, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n__________________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAttenti (None, 399, 768)     0           Encoder-11-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAttenti (None, 399, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n                                                                 Encoder-11-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAttenti (None, 399, 768)     1536        Encoder-11-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-11-FeedForward (FeedFor (None, 399, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-11-FeedForward-Dropout  (None, 399, 768)     0           Encoder-11-FeedForward[0][0]     \n__________________________________________________________________________________________________\nEncoder-11-FeedForward-Add (Add (None, 399, 768)     0           Encoder-11-MultiHeadSelfAttention\n                                                                 Encoder-11-FeedForward-Dropout[0]\n__________________________________________________________________________________________________\nEncoder-11-FeedForward-Norm (La (None, 399, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n__________________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAttenti (None, 399, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n__________________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAttenti (None, 399, 768)     0           Encoder-12-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAttenti (None, 399, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n                                                                 Encoder-12-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAttenti (None, 399, 768)     1536        Encoder-12-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-12-FeedForward (FeedFor (None, 399, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-12-FeedForward-Dropout  (None, 399, 768)     0           Encoder-12-FeedForward[0][0]     \n__________________________________________________________________________________________________\nEncoder-12-FeedForward-Add (Add (None, 399, 768)     0           Encoder-12-MultiHeadSelfAttention\n                                                                 Encoder-12-FeedForward-Dropout[0]\n__________________________________________________________________________________________________\nEncoder-12-FeedForward-Norm (La (None, 399, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n__________________________________________________________________________________________________\nExtract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n__________________________________________________________________________________________________\nNSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 2)            1538        NSP-Dense[0][0]                  \n==================================================================================================\nTotal params: 109,396,994\nTrainable params: 109,396,994\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"learner=ktrain.get_learner(model=model,train_data=(x_train,y_train),val_data=(x_test,y_test),batch_size=6)","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:21:46.379522Z","iopub.execute_input":"2022-07-10T10:21:46.380088Z","iopub.status.idle":"2022-07-10T10:21:47.454223Z","shell.execute_reply.started":"2022-07-10T10:21:46.380049Z","shell.execute_reply":"2022-07-10T10:21:47.453248Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#learner.lr_find(start_lr=1e-4,max_epochs=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:21:47.456614Z","iopub.execute_input":"2022-07-10T10:21:47.457286Z","iopub.status.idle":"2022-07-10T10:21:47.462659Z","shell.execute_reply.started":"2022-07-10T10:21:47.457248Z","shell.execute_reply":"2022-07-10T10:21:47.460497Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#learner.lr_plot()","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:21:47.464053Z","iopub.execute_input":"2022-07-10T10:21:47.464413Z","iopub.status.idle":"2022-07-10T10:21:47.626810Z","shell.execute_reply.started":"2022-07-10T10:21:47.464378Z","shell.execute_reply":"2022-07-10T10:21:47.625877Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"learner.fit_onecycle(lr=1e-4,epochs=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:21:47.628277Z","iopub.execute_input":"2022-07-10T10:21:47.628753Z","iopub.status.idle":"2022-07-10T10:58:23.218124Z","shell.execute_reply.started":"2022-07-10T10:21:47.628719Z","shell.execute_reply":"2022-07-10T10:58:23.217218Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\n\nbegin training using onecycle policy with max lr of 0.0001...\n5000/5000 [==============================] - 2195s 436ms/step - loss: 0.5147 - accuracy: 0.6834 - val_loss: 0.6931 - val_accuracy: 0.5028\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f40633b9750>"},"metadata":{}}]},{"cell_type":"code","source":"predictor=ktrain.get_predictor(learner.model,preprocess)\npredictor","metadata":{"execution":{"iopub.status.busy":"2022-07-10T10:58:24.053209Z","iopub.execute_input":"2022-07-10T10:58:24.053833Z","iopub.status.idle":"2022-07-10T10:58:24.060352Z","shell.execute_reply.started":"2022-07-10T10:58:24.053799Z","shell.execute_reply":"2022-07-10T10:58:24.059445Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"<ktrain.text.predictor.TextPredictor at 0x7f40817f1650>"},"metadata":{}}]}]}